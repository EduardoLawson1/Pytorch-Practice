{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNoweR8WirQin169AQCiXg2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EduardoLawson1/Pytorch-Practice/blob/main/Pytorch%20Transfer%20Learning/06_pytorch_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Pytorch Transfer Learning\n",
        "Transfer learning involves taking the parameters of what one model has learned on another dataset and aplying to other\n"
      ],
      "metadata": {
        "id": "3kuM2ANKqFS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(torch.__version__) # want 1.12+\n",
        "print(torchvision.__version__) # want 0.13+"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zI_QQiQ1rcE1",
        "outputId": "d2a1171f-b6c0-4674-e480-42e94e640ba6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.0+cu121\n",
            "0.20.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import the code from the going modular section"
      ],
      "metadata": {
        "id": "5NIgx1t2sDlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3LlOdaMs9k0",
        "outputId": "274eb383-51e9-4c41-81eb-f826386cbd13"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n",
            "[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\n",
            "Cloning into 'pytorch-deep-learning'...\n",
            "remote: Enumerating objects: 4356, done.\u001b[K\n",
            "remote: Counting objects: 100% (321/321), done.\u001b[K\n",
            "remote: Compressing objects: 100% (144/144), done.\u001b[K\n",
            "remote: Total 4356 (delta 213), reused 252 (delta 176), pack-reused 4035 (from 1)\u001b[K\n",
            "Receiving objects: 100% (4356/4356), 654.51 MiB | 22.79 MiB/s, done.\n",
            "Resolving deltas: 100% (2584/2584), done.\n",
            "Updating files: 100% (248/248), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device-agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qhUzNO70tbnH",
        "outputId": "848fa290-0b6c-4c07-830d-53177dfd520e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdJUzDh2C6On",
        "outputId": "39540d34-1d9a-4cb7-ea92-f65f4e3624e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 11 15:21:44 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P8              12W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Get data"
      ],
      "metadata": {
        "id": "HJcKPJ6KC8Gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "import requests\n",
        "# Setup data path\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "# if the images folder doesn't exist, download it an prepare it\n",
        "if image_path.is_dir():\n",
        "  print(f\"{image_path} directory exists, skipping download\")\n",
        "else:\n",
        "  print(f\"did not find {image_path}, downloading it\")\n",
        "  image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "   # Download dataset\n",
        "  with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "      request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "      print(\" Downloading it\")\n",
        "      f.write(request.content)\n",
        "  # Unzip dataa\n",
        "  with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "    print(\"Unzipping data\")\n",
        "    zip_ref.extractall(image_path)\n",
        "\n",
        "    # remove zip file\n",
        "  os.remove(data_path / \"pizza_steak_sushi.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoEZf_nhBO2U",
        "outputId": "5260525e-576c-4b3b-86be-fc39a0d40b24"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "did not find data/pizza_steak_sushi, downloading it\n",
            " Downloading it\n",
            "Unzipping data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup directories\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\"\n",
        "\n",
        "train_dir, test_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_hHUkKPDmIc",
        "outputId": "5a262d8f-a1f8-43e4-ef62-f2e0f0864a6d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('data/pizza_steak_sushi/train'),\n",
              " PosixPath('data/pizza_steak_sushi/test'))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create Datasets and DataLoaders\n",
        "Now we have got some data and will turn into Pytorch DataLoaders.\n",
        "\n",
        "* Important Point: When using a pretrained model, it is important that the data that you pass through it is transformed in the same way the data the model was trained on"
      ],
      "metadata": {
        "id": "AA_vWxgqD2aP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import data_setup"
      ],
      "metadata": {
        "id": "wQijINcp0MXE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Creating transforms (manual creation)\n",
        "\n",
        "Using the same setup of ImageNet"
      ],
      "metadata": {
        "id": "ZUQQtL-H1WaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "manual_transforms = transforms.Compose([\n",
        "                                        transforms.Resize((224, 224)), # Resize for height and width = 224\n",
        "                                        transforms.ToTensor(),  # Get images into range [0, 1]\n",
        "                                        normalize]) # Equal to ImageNet"
      ],
      "metadata": {
        "id": "NO2Z6aBi1Oo9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import data_setup\n",
        "\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                               test_dir=test_dir,\n",
        "                                                                               transform=manual_transforms,\n",
        "                                                                               batch_size=32)\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84jpedc72J78",
        "outputId": "49cd123b-3b9b-4697-dadc-bbcb78768ce8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7c743304b610>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7c743304b010>,\n",
              " ['pizza', 'steak', 'sushi'])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Creating a transform for `torchvision.models` (auto creation)\n"
      ],
      "metadata": {
        "id": "qUcJJ0jo3qE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a set of pretrained model weights\n",
        "\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # Default = best available\n",
        "weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HseTLMuZ4zay",
        "outputId": "a2c29974-e960-4e84-fd79-4079e9ca8767"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EfficientNet_B0_Weights.IMAGENET1K_V1"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the transformeds used to create our pretrained weights\n",
        "auto_transforms = weights.transforms()\n",
        "auto_transforms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8D0pztA5xcb",
        "outputId": "6239cc15-84a3-4e71-e262-8b8109965f5a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImageClassification(\n",
              "    crop_size=[224]\n",
              "    resize_size=[256]\n",
              "    mean=[0.485, 0.456, 0.406]\n",
              "    std=[0.229, 0.224, 0.225]\n",
              "    interpolation=InterpolationMode.BICUBIC\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders using automatic transforms\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                               test_dir=test_dir,\n",
        "                                                                               transform=auto_transforms,\n",
        "                                                                               batch_size=32)\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YX2DK8F66K_g",
        "outputId": "ac7014dc-2137-4516-d519-161a34afd853"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7c7433574520>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7c7433575360>,\n",
              " ['pizza', 'steak', 'sushi'])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Getting a pretrained model\n",
        "\n",
        "There are various places to get a pretrained model, such as:\n",
        "1. Pytorch domain libraries\n",
        "2. Libraries like timm (torch image models)\n",
        "3. Hugging face\n",
        "4. Papers with code"
      ],
      "metadata": {
        "id": "wW9UgoRl7LwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Wich pretrained model should  i use?\n",
        "*Experimenting is the best*\n",
        "\n",
        "Three things to consider?\n",
        "1. Speed - how fast does it run?\n",
        "2. Size - how big it is?\n",
        "3. Performance - how well it works?\n",
        "\n",
        "For my case(deploying Food Vision Mini on a mobile device), it looks like EffNetB0 is one of our best options in terms of performance vs size.\n",
        "\n",
        "However, in light of the Bitter Lesson, if we had infinite compute we'd likely pick the biggest model + most parameters + most general we could"
      ],
      "metadata": {
        "id": "9Uawh2lv6Wnm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B2ei4UVl95Pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h4QEzod49n1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gribcRrE8IAO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}